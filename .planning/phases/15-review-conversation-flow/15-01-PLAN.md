---
phase: 15-review-conversation-flow
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/graph/intent.py
  - src/schemas/state.py
  - src/graph/nodes/review_continuation.py
  - src/graph/graph.py
  - src/slack/handlers.py
  - tests/test_intent_continuation.py
autonomous: true
---

<objective>
Context-aware intent classification for review continuations.

Purpose: When user replies to a REVIEW with answers to open questions, classify as REVIEW_CONTINUATION instead of TICKET. This prevents structured answers from being misclassified as ticket creation requests.

Output:
- REVIEW_CONTINUATION intent type with continuation patterns
- Context-aware classification in `classify_intent()` using `has_review_context`
- LLM fallback with review context awareness
- `review_continuation_node` that synthesizes answers and asks for approval
- Handler dispatch for review continuation responses
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/STATE.md
@.planning/phases/15-review-conversation-flow/15-CONTEXT.md
@src/graph/intent.py
@src/graph/nodes/review.py
@src/graph/graph.py
@src/schemas/state.py
@src/slack/handlers.py
</context>

<tasks>
<task type="auto">
  <name>Task 1: Add REVIEW_CONTINUATION intent type and patterns</name>
  <files>src/graph/intent.py</files>
  <action>
1. Add REVIEW_CONTINUATION to IntentType enum:
   ```python
   REVIEW_CONTINUATION = "REVIEW_CONTINUATION"  # Answering questions from previous review
   ```

2. Add REVIEW_CONTINUATION_PATTERNS list (for pattern-based detection):
   ```python
   # Review continuation patterns - user answering questions from previous review
   # These are checked ONLY when has_review_context=True
   REVIEW_CONTINUATION_PATTERNS = [
       # Direct answers (key-value style)
       (r"^[\w\s]+\s*[-:]\s*\w+", "pattern: key-value answer format"),
       # Numbered answers
       (r"^\d+[.)]\s*\w+", "pattern: numbered answer"),
       # Bullet answers
       (r"^[-•]\s*\w+", "pattern: bullet answer"),
       # "For X, I'd choose Y" pattern
       (r"\bfor\s+\w+.*(?:choose|select|go with|use)\b", "pattern: for X choose Y"),
       # Multiple comma-separated items (answers to multiple questions)
       (r"^[\w\s]+,\s*[\w\s]+,\s*[\w\s]+", "pattern: comma-separated answers"),
   ]

   # NOT continuation patterns - these override continuation detection
   NOT_CONTINUATION_PATTERNS = [
       (r"\bcreate\s+(?:a\s+)?(?:new\s+)?ticket\b", "pattern: create new ticket"),
       (r"\bnew\s+(?:ticket|task|story|feature|bug)\b", "pattern: new ticket/task"),
       (r"\bpropose\s+(?:new|another|different)\b", "pattern: propose new/different"),
       (r"\blet'?s?\s+start\s+(?:over|fresh|new)\b", "pattern: start over"),
       (r"\bactually\s*,?\s*(?:I\s+)?(?:want|need)\s+(?:a\s+)?(?:new|different)\b", "pattern: actually want new/different"),
   ]
   ```

3. Add `_check_review_continuation()` helper function:
   ```python
   def _check_review_continuation(message: str) -> Optional[IntentResult]:
       """Check if message looks like answers to review questions.

       Called only when has_review_context=True.
       Returns IntentResult if continuation detected, None otherwise.
       """
       message_lower = message.lower().strip()

       # First check NOT_CONTINUATION patterns (these override)
       for pattern, reason in NOT_CONTINUATION_PATTERNS:
           if re.search(pattern, message_lower, re.IGNORECASE):
               # Explicit new request - not a continuation
               return None

       # Check continuation patterns
       for pattern, reason in REVIEW_CONTINUATION_PATTERNS:
           if re.search(pattern, message, re.IGNORECASE):  # Use original case for some patterns
               return IntentResult(
                   intent=IntentType.REVIEW_CONTINUATION,
                   confidence=0.9,  # High but not 1.0 to allow LLM override
                   reasons=[reason],
               )

       return None
   ```

4. Update `classify_intent_patterns()` to check continuation patterns when has_review_context=True:
   - After NEGATION patterns check (before TICKET_ACTION)
   - Only when has_review_context=True
   ```python
   # Check REVIEW_CONTINUATION patterns (when in review context)
   if has_review_context:
       continuation_result = _check_review_continuation(message)
       if continuation_result is not None:
           return continuation_result
   ```

5. Update LLM classification prompt to be context-aware:
   - Create INTENT_PROMPT_WITH_REVIEW_CONTEXT for when has_review_context=True
   - Bias toward REVIEW_CONTINUATION when user provides structured answers
   ```python
   INTENT_PROMPT_WITH_REVIEW_CONTEXT = """Classify this user message. IMPORTANT: This message is a reply in a thread
   where the bot just provided an architecture review with open questions.

   User message: "{message}"

   If the message looks like answers to questions (e.g., "Option A", "Yes",
   key-value pairs, bullet points, comma-separated choices), classify as REVIEW_CONTINUATION.

   Only classify as TICKET if user explicitly asks for a new ticket.

   Categories:
   - REVIEW_CONTINUATION: Answering questions from previous review
   - DECISION_APPROVAL: Approving the reviewed approach ("let's go with this", "approved")
   - TICKET: Explicitly requesting a new Jira ticket
   - DISCUSSION: General conversation, clarifying question

   Respond in this exact format:
   INTENT: <REVIEW_CONTINUATION|DECISION_APPROVAL|TICKET|DISCUSSION>
   CONFIDENCE: <0.0-1.0>
   REASON: <brief explanation>
   """
   ```

6. Update `_llm_classify()` to accept has_review_context parameter and use appropriate prompt:
   ```python
   async def _llm_classify(message: str, has_review_context: bool = False) -> IntentResult:
   ```
  </action>
  <verify>python -c "from src.graph.intent import IntentType, REVIEW_CONTINUATION_PATTERNS; print(IntentType.REVIEW_CONTINUATION, len(REVIEW_CONTINUATION_PATTERNS))"</verify>
  <done>REVIEW_CONTINUATION intent type exists, continuation patterns work, LLM prompt updated</done>
</task>

<task type="auto">
  <name>Task 2: Create review_continuation_node</name>
  <files>src/graph/nodes/review_continuation.py, src/schemas/state.py</files>
  <action>
1. Create new file `src/graph/nodes/review_continuation.py`:
   ```python
   """Review continuation node - synthesize user answers and update recommendations.

   When user provides answers to open questions from a review, this node:
   1. Maps answers to original questions
   2. Updates architecture recommendations
   3. Asks user to proceed with decision
   """
   import logging
   from typing import Any

   from src.schemas.state import AgentState

   logger = logging.getLogger(__name__)


   REVIEW_CONTINUATION_PROMPT = '''You are continuing an architecture discussion.

   Original review topic: {topic}

   Your previous analysis:
   {original_review}

   User's answers to your open questions:
   {user_answers}

   Based on these answers, provide:
   1. Brief acknowledgment of their choices (1-2 sentences)
   2. How these choices affect the architecture (2-3 key implications)
   3. Your updated recommendation (which option to proceed with)
   4. A clear question asking if they want to proceed

   Format for Slack:
   - Bold: *text*
   - Lists: Use bullet •
   - Keep it concise (not as long as original review)
   - End with a clear "Should we proceed with [approach]?" question
   '''


   async def review_continuation_node(state: AgentState) -> dict[str, Any]:
       """Continue review conversation after user provides answers.

       Args:
           state: Current AgentState dict

       Returns:
           Partial state update with decision_result and updated review_context
       """
       from langchain_core.messages import HumanMessage
       from src.llm import get_llm

       review_context = state.get("review_context")

       if not review_context:
           logger.warning("review_continuation_node called without review_context")
           return {
               "decision_result": {
                   "action": "review_continuation",
                   "message": "I don't have context from a previous review. What would you like me to analyze?",
               }
           }

       # Get latest human message (user's answers)
       messages = state.get("messages", [])
       latest_human_message = ""

       for msg in reversed(messages):
           if isinstance(msg, HumanMessage):
               latest_human_message = msg.content
               break

       # Get review context fields
       topic = review_context.get("topic", "Architecture discussion")
       original_review = review_context.get("review_summary", "")
       persona = review_context.get("persona", "Architect")

       # Build prompt
       prompt = REVIEW_CONTINUATION_PROMPT.format(
           topic=topic,
           original_review=original_review,
           user_answers=latest_human_message,
       )

       # Call LLM
       llm = get_llm()
       try:
           continuation = await llm.chat(prompt)

           logger.info(
               "Review continuation generated",
               extra={
                   "topic": topic,
                   "persona": persona,
                   "answers_length": len(latest_human_message),
                   "continuation_length": len(continuation),
               },
           )

           # Update review_context with answers received
           # Keep context so DECISION_APPROVAL can still trigger
           updated_context = {
               **review_context,
               "answers_received": True,
               "updated_recommendation": continuation,
           }

           return {
               "decision_result": {
                   "action": "review_continuation",
                   "message": continuation,
                   "persona": persona,
                   "topic": topic,
               },
               "review_context": updated_context,
           }

       except Exception as e:
           logger.error(f"Review continuation LLM call failed: {e}")
           return {
               "decision_result": {
                   "action": "review_continuation",
                   "message": f"I encountered an error processing your answers: {str(e)}",
               }
           }
   ```

2. Update `src/graph/nodes/__init__.py` to export review_continuation_node (if __init__.py exists, otherwise skip)

3. No state.py changes needed - review_context already supports this flow
  </action>
  <verify>python -c "from src.graph.nodes.review_continuation import review_continuation_node; print('OK')"</verify>
  <done>review_continuation_node created, synthesizes answers, returns continuation response</done>
</task>

<task type="auto">
  <name>Task 3: Wire review_continuation into graph routing</name>
  <files>src/graph/graph.py</files>
  <action>
1. Import review_continuation_node:
   ```python
   from src.graph.nodes.review_continuation import review_continuation_node
   ```

2. Add node to graph:
   ```python
   workflow.add_node("review_continuation", review_continuation_node)
   ```

3. Update route_after_intent() to handle REVIEW_CONTINUATION:
   ```python
   def route_after_intent(state: AgentState) -> Literal["ticket_flow", "review_flow", "discussion_flow", "ticket_action_flow", "decision_approval_flow", "review_continuation_flow"]:
       ...
       elif intent == "REVIEW_CONTINUATION":
           logger.info("Intent router: routing to review_continuation_flow")
           return "review_continuation_flow"
   ```

4. Update conditional edges to include review_continuation:
   ```python
   workflow.add_conditional_edges(
       "intent_router",
       route_after_intent,
       {
           "ticket_flow": "extraction",
           "review_flow": "review",
           "discussion_flow": "discussion",
           "ticket_action_flow": "ticket_action",
           "decision_approval_flow": "decision_approval",
           "review_continuation_flow": "review_continuation",  # NEW
       }
   )
   ```

5. Add edge from review_continuation to END:
   ```python
   workflow.add_edge("review_continuation", END)
   ```
  </action>
  <verify>python -c "from src.graph.graph import create_graph; g = create_graph(); print('Nodes:', list(g.nodes.keys()))"</verify>
  <done>Graph routing includes review_continuation flow</done>
</task>

<task type="auto">
  <name>Task 4: Add handler dispatch for review_continuation</name>
  <files>src/slack/handlers.py</files>
  <action>
1. Add handler dispatch for action == "review_continuation":
   ```python
   elif action == "review_continuation":
       # Review continuation - synthesized response to user's answers
       # Similar to review, but more concise (follow-up, not initial analysis)
       continuation_msg = result.get("message", "")
       persona = result.get("persona", "")

       if persona:
           prefix = f"*{persona}:*\n\n"
       else:
           prefix = ""

       if continuation_msg:
           full_text = prefix + continuation_msg

           # Build blocks (similar to review, but no chunking needed for shorter response)
           blocks = [
               {
                   "type": "section",
                   "text": {"type": "mrkdwn", "text": full_text[:2900]}  # Truncate if needed
               }
           ]

           # Post continuation response
           client.chat_postMessage(
               channel=identity.channel_id,
               thread_ts=identity.thread_ts if identity.thread_ts else None,
               blocks=blocks,
               text=full_text[:200],  # Fallback text
           )
   ```

   Place this BEFORE the `elif action == "review"` block (order matters for readability)
  </action>
  <verify>grep -n "review_continuation" src/slack/handlers.py</verify>
  <done>Handler dispatches review_continuation with formatted response</done>
</task>

<task type="auto">
  <name>Task 5: Add tests for review continuation intent classification</name>
  <files>tests/test_intent_continuation.py</files>
  <action>
1. Create new test file `tests/test_intent_continuation.py`:
   ```python
   """Tests for REVIEW_CONTINUATION intent classification.

   Phase 15: When user replies to a REVIEW with answers, should classify as
   REVIEW_CONTINUATION (not TICKET).
   """
   import pytest
   from src.graph.intent import (
       classify_intent_patterns,
       IntentType,
       IntentResult,
   )


   class TestReviewContinuationPatterns:
       """Test REVIEW_CONTINUATION detection with has_review_context=True."""

       # Key-value format answers
       @pytest.mark.parametrize("message", [
           "Provisioning Type - Automatic",
           "IdP: Okta",
           "Security - Employee loop",
           "Granularity - system access",
       ])
       def test_key_value_answers(self, message):
           """Key-value style answers should be REVIEW_CONTINUATION."""
           result = classify_intent_patterns(message, has_review_context=True)
           assert result is not None
           assert result.intent == IntentType.REVIEW_CONTINUATION

       # Multiple comma-separated answers
       @pytest.mark.parametrize("message", [
           "Automatic, Okta, system access, Employee",
           "Option A, standard flow, weekly updates",
       ])
       def test_comma_separated_answers(self, message):
           """Comma-separated answers should be REVIEW_CONTINUATION."""
           result = classify_intent_patterns(message, has_review_context=True)
           assert result is not None
           assert result.intent == IntentType.REVIEW_CONTINUATION

       # Numbered answers
       @pytest.mark.parametrize("message", [
           "1. Automatic provisioning",
           "1) Okta for IdP",
           "2. System-level access",
       ])
       def test_numbered_answers(self, message):
           """Numbered answers should be REVIEW_CONTINUATION."""
           result = classify_intent_patterns(message, has_review_context=True)
           assert result is not None
           assert result.intent == IntentType.REVIEW_CONTINUATION

       # Bullet answers
       @pytest.mark.parametrize("message", [
           "- Automatic provisioning",
           "• Okta integration",
       ])
       def test_bullet_answers(self, message):
           """Bullet-point answers should be REVIEW_CONTINUATION."""
           result = classify_intent_patterns(message, has_review_context=True)
           assert result is not None
           assert result.intent == IntentType.REVIEW_CONTINUATION


   class TestReviewContinuationWithoutContext:
       """Without review context, same messages should NOT be REVIEW_CONTINUATION."""

       @pytest.mark.parametrize("message", [
           "Provisioning Type - Automatic",
           "IdP: Okta",
           "1. Automatic provisioning",
       ])
       def test_no_continuation_without_context(self, message):
           """Without has_review_context, should NOT match continuation patterns."""
           result = classify_intent_patterns(message, has_review_context=False)
           # Should either be None (fall to LLM) or different intent
           if result is not None:
               assert result.intent != IntentType.REVIEW_CONTINUATION


   class TestNotContinuationPatterns:
       """Test that explicit new requests override continuation detection."""

       @pytest.mark.parametrize("message", [
           "Create a new ticket for this",
           "Let's create a ticket",
           "I need a new task for deployment",
           "Propose a different architecture",
           "Let's start over with a fresh approach",
           "Actually, I want a new feature request",
       ])
       def test_explicit_new_requests_not_continuation(self, message):
           """Explicit new ticket/task requests should NOT be REVIEW_CONTINUATION."""
           result = classify_intent_patterns(message, has_review_context=True)
           # Should either be None (fall to LLM) or be TICKET intent
           if result is not None:
               assert result.intent != IntentType.REVIEW_CONTINUATION


   class TestDecisionApprovalStillWorks:
       """Decision approval patterns should still work with review context."""

       @pytest.mark.parametrize("message", [
           "Let's go with this",
           "Approved",
           "Sounds good, let's proceed",
           "Ship it",
       ])
       def test_decision_approval_patterns(self, message):
           """Decision approval patterns should still detect DECISION_APPROVAL."""
           result = classify_intent_patterns(message, has_review_context=True)
           assert result is not None
           assert result.intent == IntentType.DECISION_APPROVAL
   ```
  </action>
  <verify>cd /Users/boris/github.com/reqirements-orchestrator && python -m pytest tests/test_intent_continuation.py -v --tb=short 2>&1 | head -50</verify>
  <done>Tests pass for REVIEW_CONTINUATION pattern matching</done>
</task>
</tasks>

<verification>
Before declaring plan complete:
- [ ] `python -c "from src.graph.intent import IntentType; print(IntentType.REVIEW_CONTINUATION)"` works
- [ ] `python -c "from src.graph.nodes.review_continuation import review_continuation_node; print('OK')"` works
- [ ] `python -c "from src.graph.graph import create_graph; print('OK')"` works
- [ ] `grep -r "review_continuation" src/` shows function in graph, node, and handlers
- [ ] `python -m pytest tests/test_intent_continuation.py -v` passes
- [ ] No Python syntax errors or import issues
</verification>

<success_criteria>
- REVIEW_CONTINUATION intent type with continuation patterns
- Context-aware classification biases toward REVIEW_CONTINUATION when has_review_context=True
- NOT_CONTINUATION patterns prevent false positives for explicit new requests
- review_continuation_node synthesizes answers and asks for approval
- Graph routes REVIEW_CONTINUATION to continuation node
- Handler dispatches continuation response with persona prefix
- Tests verify pattern matching for common answer formats
</success_criteria>

<output>
After completion, create `.planning/phases/15-review-conversation-flow/15-01-SUMMARY.md`
</output>
