---
phase: 03-llm-integration
plan: 04
type: execute
wave: 2
depends_on: ["03-01"]
files_modified: [src/llm/adapters/anthropic.py, src/llm/adapters/__init__.py, src/config/settings.py, pyproject.toml]
autonomous: true
---

<objective>
Create Anthropic provider adapter using langchain-anthropic.

Purpose: Implement Anthropic/Claude-specific logic while conforming to unified interfaces.
Output: AnthropicAdapter that converts Anthropic responses to LLMResult format.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

Architecture decisions:
- Provider-specific adapter with unified result format
- Standard logging: request_id, provider, model, latency

Prior plan 03-01 provides:
- LLMProvider, Message, ToolCall, LLMResult, LLMConfig types
- BaseAdapter interface

Note: Anthropic handles system messages differently (separate parameter, not in messages list).

@src/llm/types.py
@src/llm/adapters/base.py
@src/config/settings.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add langchain-anthropic dependency</name>
  <files>pyproject.toml</files>
  <action>
Add langchain-anthropic to dependencies in pyproject.toml:
- "langchain-anthropic>=0.2"
  </action>
  <verify>pip install -e . && python -c "from langchain_anthropic import ChatAnthropic; print('langchain-anthropic ok')"</verify>
  <done>langchain-anthropic dependency installed</done>
</task>

<task type="auto">
  <name>Task 2: Add Anthropic settings</name>
  <files>src/config/settings.py</files>
  <action>
Add Anthropic configuration to Settings class:

```python
# Anthropic (optional - for multi-provider support)
anthropic_api_key: Optional[str] = None
```

Add to .env.example as well:
```
# Anthropic (optional)
ANTHROPIC_API_KEY=your-anthropic-api-key
```
  </action>
  <verify>python -c "from src.config import get_settings; s = get_settings(); print('anthropic_api_key field exists' if hasattr(s, 'anthropic_api_key') else 'missing')"</verify>
  <done>Anthropic API key in settings</done>
</task>

<task type="auto">
  <name>Task 3: Create Anthropic adapter</name>
  <files>src/llm/adapters/anthropic.py</files>
  <action>
Implement AnthropicAdapter using langchain-anthropic.

Note: Anthropic handles system messages as a separate parameter, not in the messages list.
The adapter needs to extract system messages and pass them separately.

```python
import time
import logging
from typing import Any, Optional
from pydantic import BaseModel

from langchain_anthropic import ChatAnthropic
from langchain_core.messages import (
    HumanMessage, AIMessage, SystemMessage, ToolMessage,
    BaseMessage
)

from src.llm.types import (
    Message, MessageRole, LLMResult, LLMConfig, LLMProvider,
    ToolCall, TokenUsage, FinishReason
)
from src.llm.adapters.base import BaseAdapter, ToolDefinition
from src.config import get_settings

logger = logging.getLogger(__name__)

class AnthropicAdapter(BaseAdapter):
    """Anthropic provider adapter using langchain-anthropic."""

    def __init__(self, config: LLMConfig):
        super().__init__(config)
        settings = get_settings()
        api_key = config.api_key or settings.anthropic_api_key
        if not api_key:
            raise ValueError("Anthropic API key required")

        self.client = ChatAnthropic(
            model=config.model,
            api_key=api_key,
            temperature=config.temperature,
            max_tokens=config.max_tokens,
            timeout=config.timeout_seconds,
        )

    def convert_messages(self, messages: list[Message]) -> tuple[str | None, list[BaseMessage]]:
        """Convert canonical messages to LangChain format.

        Returns (system_message, other_messages) because Anthropic
        handles system messages separately.
        """
        system_content = None
        result = []

        for msg in messages:
            if msg.role == MessageRole.SYSTEM:
                # Anthropic: system message is separate, use last one if multiple
                system_content = msg.content
            elif msg.role == MessageRole.USER:
                result.append(HumanMessage(content=msg.content))
            elif msg.role == MessageRole.ASSISTANT:
                result.append(AIMessage(content=msg.content))
            elif msg.role == MessageRole.TOOL:
                result.append(ToolMessage(
                    content=msg.content,
                    tool_call_id=msg.tool_call_id or "",
                    name=msg.name or ""
                ))

        return system_content, result

    def _convert_tools(self, tools: list[ToolDefinition]) -> list[dict]:
        """Convert tool definitions to Anthropic format."""
        return [
            {
                "name": t.name,
                "description": t.description,
                "input_schema": t.parameters,
            }
            for t in tools
        ]

    def parse_response(self, response: Any, latency_ms: float) -> LLMResult:
        """Parse LangChain response to unified format."""
        text = response.content if hasattr(response, 'content') else ""

        # Handle Anthropic's content blocks (text and tool_use)
        if isinstance(text, list):
            # Extract text from content blocks
            text_parts = []
            for block in text:
                if isinstance(block, dict) and block.get("type") == "text":
                    text_parts.append(block.get("text", ""))
                elif isinstance(block, str):
                    text_parts.append(block)
            text = "".join(text_parts)

        tool_calls = []
        if hasattr(response, 'tool_calls') and response.tool_calls:
            for tc in response.tool_calls:
                tool_calls.append(ToolCall(
                    id=tc.get("id", ""),
                    name=tc.get("name", ""),
                    arguments=tc.get("args", {}),
                ))

        finish_reason = FinishReason.TOOL_CALLS if tool_calls else FinishReason.STOP

        usage = TokenUsage()
        if hasattr(response, 'usage_metadata') and response.usage_metadata:
            um = response.usage_metadata
            usage = TokenUsage(
                prompt_tokens=um.get("input_tokens", 0),
                completion_tokens=um.get("output_tokens", 0),
                total_tokens=um.get("total_tokens", 0),
            )

        return LLMResult(
            text=text if isinstance(text, str) else "",
            tool_calls=tool_calls,
            finish_reason=finish_reason,
            provider=LLMProvider.ANTHROPIC,
            model=self.config.model,
            latency_ms=latency_ms,
            usage=usage,
            raw=response,
        )

    async def invoke(
        self,
        messages: list[Message],
        tools: list[ToolDefinition] | None = None,
        response_schema: type[BaseModel] | None = None,
    ) -> LLMResult:
        """Send messages to Anthropic and get unified result."""
        start_time = time.perf_counter()

        try:
            system_msg, lc_messages = self.convert_messages(messages)

            # Anthropic needs at least one non-system message
            if not lc_messages:
                lc_messages = [HumanMessage(content="Hello")]

            client = self.client
            if tools:
                client = client.bind_tools(self._convert_tools(tools))

            if response_schema:
                client = client.with_structured_output(response_schema)

            # Invoke with system message if present
            invoke_kwargs = {}
            if system_msg:
                # LangChain handles this via config, not separate param
                pass

            response = await client.ainvoke(lc_messages)

            latency_ms = (time.perf_counter() - start_time) * 1000

            result = self.parse_response(response, latency_ms)

            logger.info(
                "Anthropic request completed",
                extra={
                    "request_id": result.request_id,
                    "provider": result.provider.value,
                    "model": result.model,
                    "latency_ms": result.latency_ms,
                }
            )

            return result

        except Exception as e:
            latency_ms = (time.perf_counter() - start_time) * 1000
            logger.error(f"Anthropic request failed: {e}")
            return LLMResult(
                text="",
                finish_reason=FinishReason.ERROR,
                provider=LLMProvider.ANTHROPIC,
                model=self.config.model,
                latency_ms=latency_ms,
            )
```
  </action>
  <verify>python -c "from src.llm.adapters.anthropic import AnthropicAdapter; print('anthropic adapter ok')"</verify>
  <done>AnthropicAdapter with Anthropic-specific message handling</done>
</task>

<task type="auto">
  <name>Task 4: Update adapters package exports</name>
  <files>src/llm/adapters/__init__.py</files>
  <action>
Add AnthropicAdapter to src/llm/adapters/__init__.py exports.

Keep existing exports.
  </action>
  <verify>python -c "from src.llm.adapters import AnthropicAdapter; print('export ok')"</verify>
  <done>AnthropicAdapter exported from adapters package</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `pip install -e .` succeeds with langchain-anthropic
- [ ] `python -c "from src.llm.adapters import AnthropicAdapter"` succeeds
- [ ] Settings has anthropic_api_key field
- [ ] No import errors
</verification>

<success_criteria>

- All tasks completed
- AnthropicAdapter implements BaseAdapter interface
- System message handling respects Anthropic's requirements
- Standard logging in place
</success_criteria>

<output>
After completion, create `.planning/phases/03-llm-integration/03-04-SUMMARY.md`
</output>
