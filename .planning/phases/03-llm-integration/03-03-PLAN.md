---
phase: 03-llm-integration
plan: 03
type: execute
wave: 2
depends_on: ["03-01"]
files_modified: [src/llm/adapters/openai.py, src/llm/adapters/__init__.py, pyproject.toml]
autonomous: true
---

<objective>
Create OpenAI provider adapter using langchain-openai.

Purpose: Implement OpenAI-specific logic while conforming to unified interfaces.
Output: OpenAIAdapter that converts OpenAI responses to LLMResult format.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

Architecture decisions:
- Provider-specific adapter with unified result format
- Standard logging: request_id, provider, model, latency

Prior plan 03-01 provides:
- LLMProvider, Message, ToolCall, LLMResult, LLMConfig types
- BaseAdapter interface

@src/llm/types.py
@src/llm/adapters/base.py
@src/config/settings.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add langchain-openai dependency</name>
  <files>pyproject.toml</files>
  <action>
Add langchain-openai to dependencies in pyproject.toml:
- "langchain-openai>=0.2"

Also add openai_api_key to Settings class if not present.
  </action>
  <verify>pip install -e . && python -c "from langchain_openai import ChatOpenAI; print('langchain-openai ok')"</verify>
  <done>langchain-openai dependency installed</done>
</task>

<task type="auto">
  <name>Task 2: Add OpenAI settings</name>
  <files>src/config/settings.py</files>
  <action>
Add OpenAI configuration to Settings class:

```python
# OpenAI (optional - for multi-provider support)
openai_api_key: Optional[str] = None
```

Add to .env.example as well:
```
# OpenAI (optional)
OPENAI_API_KEY=your-openai-api-key
```
  </action>
  <verify>python -c "from src.config import get_settings; s = get_settings(); print('openai_api_key field exists' if hasattr(s, 'openai_api_key') else 'missing')"</verify>
  <done>OpenAI API key in settings</done>
</task>

<task type="auto">
  <name>Task 3: Create OpenAI adapter</name>
  <files>src/llm/adapters/openai.py</files>
  <action>
Implement OpenAIAdapter using langchain-openai.

```python
import time
import logging
from typing import Any, Optional
from pydantic import BaseModel

from langchain_openai import ChatOpenAI
from langchain_core.messages import (
    HumanMessage, AIMessage, SystemMessage, ToolMessage,
    BaseMessage
)

from src.llm.types import (
    Message, MessageRole, LLMResult, LLMConfig, LLMProvider,
    ToolCall, TokenUsage, FinishReason
)
from src.llm.adapters.base import BaseAdapter, ToolDefinition
from src.config import get_settings

logger = logging.getLogger(__name__)

class OpenAIAdapter(BaseAdapter):
    """OpenAI provider adapter using langchain-openai."""

    def __init__(self, config: LLMConfig):
        super().__init__(config)
        settings = get_settings()
        api_key = config.api_key or settings.openai_api_key
        if not api_key:
            raise ValueError("OpenAI API key required")

        self.client = ChatOpenAI(
            model=config.model,
            api_key=api_key,
            temperature=config.temperature,
            max_tokens=config.max_tokens,
            timeout=config.timeout_seconds,
        )

    def convert_messages(self, messages: list[Message]) -> list[BaseMessage]:
        """Convert canonical messages to LangChain format."""
        result = []
        for msg in messages:
            if msg.role == MessageRole.SYSTEM:
                result.append(SystemMessage(content=msg.content))
            elif msg.role == MessageRole.USER:
                result.append(HumanMessage(content=msg.content))
            elif msg.role == MessageRole.ASSISTANT:
                result.append(AIMessage(content=msg.content))
            elif msg.role == MessageRole.TOOL:
                result.append(ToolMessage(
                    content=msg.content,
                    tool_call_id=msg.tool_call_id or "",
                    name=msg.name or ""
                ))
        return result

    def _convert_tools(self, tools: list[ToolDefinition]) -> list[dict]:
        """Convert tool definitions to OpenAI format."""
        return [
            {
                "type": "function",
                "function": {
                    "name": t.name,
                    "description": t.description,
                    "parameters": t.parameters,
                }
            }
            for t in tools
        ]

    def parse_response(self, response: Any, latency_ms: float) -> LLMResult:
        """Parse LangChain response to unified format."""
        text = response.content if hasattr(response, 'content') else ""

        tool_calls = []
        if hasattr(response, 'tool_calls') and response.tool_calls:
            for tc in response.tool_calls:
                tool_calls.append(ToolCall(
                    id=tc.get("id", ""),
                    name=tc.get("name", ""),
                    arguments=tc.get("args", {}),
                ))

        finish_reason = FinishReason.TOOL_CALLS if tool_calls else FinishReason.STOP

        usage = TokenUsage()
        if hasattr(response, 'usage_metadata') and response.usage_metadata:
            um = response.usage_metadata
            usage = TokenUsage(
                prompt_tokens=um.get("input_tokens", 0),
                completion_tokens=um.get("output_tokens", 0),
                total_tokens=um.get("total_tokens", 0),
            )

        return LLMResult(
            text=text,
            tool_calls=tool_calls,
            finish_reason=finish_reason,
            provider=LLMProvider.OPENAI,
            model=self.config.model,
            latency_ms=latency_ms,
            usage=usage,
            raw=response,
        )

    async def invoke(
        self,
        messages: list[Message],
        tools: list[ToolDefinition] | None = None,
        response_schema: type[BaseModel] | None = None,
    ) -> LLMResult:
        """Send messages to OpenAI and get unified result."""
        start_time = time.perf_counter()

        try:
            lc_messages = self.convert_messages(messages)

            client = self.client
            if tools:
                client = client.bind_tools(self._convert_tools(tools))

            if response_schema:
                client = client.with_structured_output(response_schema)

            response = await client.ainvoke(lc_messages)

            latency_ms = (time.perf_counter() - start_time) * 1000

            result = self.parse_response(response, latency_ms)

            logger.info(
                "OpenAI request completed",
                extra={
                    "request_id": result.request_id,
                    "provider": result.provider.value,
                    "model": result.model,
                    "latency_ms": result.latency_ms,
                }
            )

            return result

        except Exception as e:
            latency_ms = (time.perf_counter() - start_time) * 1000
            logger.error(f"OpenAI request failed: {e}")
            return LLMResult(
                text="",
                finish_reason=FinishReason.ERROR,
                provider=LLMProvider.OPENAI,
                model=self.config.model,
                latency_ms=latency_ms,
            )
```
  </action>
  <verify>python -c "from src.llm.adapters.openai import OpenAIAdapter; print('openai adapter ok')"</verify>
  <done>OpenAIAdapter with message conversion, tool support, and logging</done>
</task>

<task type="auto">
  <name>Task 4: Update adapters package exports</name>
  <files>src/llm/adapters/__init__.py</files>
  <action>
Add OpenAIAdapter to src/llm/adapters/__init__.py exports.

Keep existing exports (BaseAdapter, ToolDefinition, GeminiAdapter if present).
  </action>
  <verify>python -c "from src.llm.adapters import OpenAIAdapter; print('export ok')"</verify>
  <done>OpenAIAdapter exported from adapters package</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `pip install -e .` succeeds with langchain-openai
- [ ] `python -c "from src.llm.adapters import OpenAIAdapter"` succeeds
- [ ] Settings has openai_api_key field
- [ ] No import errors
</verification>

<success_criteria>

- All tasks completed
- OpenAIAdapter implements BaseAdapter interface
- Message and tool conversion matches OpenAI format
- Standard logging in place
</success_criteria>

<output>
After completion, create `.planning/phases/03-llm-integration/03-03-SUMMARY.md`
</output>
