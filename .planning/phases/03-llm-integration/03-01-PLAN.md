---
phase: 03-llm-integration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified: [src/llm/__init__.py, src/llm/gemini.py]
autonomous: true
---

<objective>
Create Gemini LLM client using langchain-google-genai for the analyst agent.

Purpose: Provide LLM capabilities for requirement extraction, validation, and question generation.
Output: GeminiClient class with chat completion and structured output support.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

Prior work:
- Settings has google_api_key: str and default_llm_model: str = "gemini-1.5-flash"
- langchain-google-genai already in pyproject.toml dependencies

@src/config/settings.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Gemini client module</name>
  <files>src/llm/gemini.py</files>
  <action>
Create Gemini LLM client using langchain-google-genai.

Implementation:
1. Import ChatGoogleGenerativeAI from langchain_google_genai
2. Create get_gemini_llm() function that returns configured ChatGoogleGenerativeAI
3. Use settings.google_api_key and settings.default_llm_model
4. Support temperature parameter (default 0.7)
5. Support max_output_tokens parameter (default 4096)

Pattern:
```python
from langchain_google_genai import ChatGoogleGenerativeAI
from src.config import get_settings

def get_gemini_llm(
    model: str | None = None,
    temperature: float = 0.7,
    max_output_tokens: int = 4096
) -> ChatGoogleGenerativeAI:
    settings = get_settings()
    return ChatGoogleGenerativeAI(
        model=model or settings.default_llm_model,
        google_api_key=settings.google_api_key,
        temperature=temperature,
        max_output_tokens=max_output_tokens,
    )
```

Add a convenience function for structured output:
```python
def get_gemini_with_structured_output(
    schema: type[BaseModel],
    model: str | None = None,
) -> Runnable:
    llm = get_gemini_llm(model=model, temperature=0)
    return llm.with_structured_output(schema)
```
  </action>
  <verify>python -c "from src.llm.gemini import get_gemini_llm, get_gemini_with_structured_output; print('imports ok')"</verify>
  <done>Gemini client functions available for import</done>
</task>

<task type="auto">
  <name>Task 2: Create llm package with exports</name>
  <files>src/llm/__init__.py</files>
  <action>
Create src/llm/__init__.py that exports:
- get_gemini_llm
- get_gemini_with_structured_output

Pattern: from src.llm import get_gemini_llm
  </action>
  <verify>python -c "from src.llm import get_gemini_llm; print('package exports ok')"</verify>
  <done>Clean package exports available</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `python -c "from src.llm import get_gemini_llm"` succeeds
- [ ] `python -c "from langchain_google_genai import ChatGoogleGenerativeAI"` succeeds
- [ ] No import errors
</verification>

<success_criteria>

- All tasks completed
- Gemini client ready for use by agent
- Structured output support available
- Clean imports from src.llm
</success_criteria>

<output>
After completion, create `.planning/phases/03-llm-integration/03-01-SUMMARY.md`
</output>
