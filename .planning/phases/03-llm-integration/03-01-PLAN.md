---
phase: 03-llm-integration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified: [src/llm/__init__.py, src/llm/types.py, src/llm/capabilities.py]
autonomous: true
---

<objective>
Define core interfaces and types for the multi-provider LLM abstraction layer.

Purpose: Establish contracts that all provider adapters must implement, ensuring unified behavior across Gemini, OpenAI, and Anthropic.
Output: Type definitions, capability matrix, and base interfaces.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

Architecture decisions from discussion:
- 3 providers: Gemini, OpenAI, Anthropic
- 3-layer architecture: Factory -> UnifiedClient -> ProviderAdapters
- Provider-specific adapters with unified result format
- Capability matrix for feature detection
- Standard logging: request_id, provider, model, latency
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create core type definitions</name>
  <files>src/llm/types.py</files>
  <action>
Create canonical message and result types that abstract provider differences.

```python
from enum import Enum
from typing import Any, Optional
from pydantic import BaseModel, Field
from datetime import datetime
import uuid

class LLMProvider(str, Enum):
    """Supported LLM providers."""
    GEMINI = "gemini"
    OPENAI = "openai"
    ANTHROPIC = "anthropic"

class MessageRole(str, Enum):
    """Canonical message roles."""
    SYSTEM = "system"
    USER = "user"
    ASSISTANT = "assistant"
    TOOL = "tool"

class Message(BaseModel):
    """Canonical message format."""
    role: MessageRole
    content: str
    name: Optional[str] = None  # For tool messages
    tool_call_id: Optional[str] = None  # For tool responses

class ToolCall(BaseModel):
    """Normalized tool call from LLM."""
    id: str
    name: str
    arguments: dict[str, Any]

class FinishReason(str, Enum):
    """Why the LLM stopped generating."""
    STOP = "stop"
    TOOL_CALLS = "tool_calls"
    LENGTH = "length"
    CONTENT_FILTER = "content_filter"
    ERROR = "error"

class TokenUsage(BaseModel):
    """Token usage tracking."""
    prompt_tokens: int = 0
    completion_tokens: int = 0
    total_tokens: int = 0

class LLMResult(BaseModel):
    """Unified result from any provider."""
    # Content
    text: str = ""
    tool_calls: list[ToolCall] = Field(default_factory=list)
    finish_reason: FinishReason = FinishReason.STOP

    # Metadata
    request_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    provider: LLMProvider
    model: str

    # Observability
    latency_ms: float = 0
    usage: TokenUsage = Field(default_factory=TokenUsage)
    timestamp: datetime = Field(default_factory=datetime.utcnow)

    # Raw response for debugging
    raw: Optional[Any] = None

class LLMConfig(BaseModel):
    """Configuration for LLM client."""
    provider: LLMProvider
    model: str
    temperature: float = 0.7
    max_tokens: int = 4096
    timeout_seconds: float = 30.0
    api_key: Optional[str] = None  # Override from settings
```
  </action>
  <verify>python -c "from src.llm.types import LLMProvider, Message, LLMResult, LLMConfig; print('types ok')"</verify>
  <done>Core types defined: LLMProvider, Message, ToolCall, LLMResult, LLMConfig</done>
</task>

<task type="auto">
  <name>Task 2: Create capability matrix</name>
  <files>src/llm/capabilities.py</files>
  <action>
Define capability matrix for feature detection across providers.

```python
from dataclasses import dataclass
from src.llm.types import LLMProvider

@dataclass(frozen=True)
class ProviderCapabilities:
    """What a provider can do."""
    tools: bool = False  # Function/tool calling
    json_schema: bool = False  # Structured output with schema
    vision: bool = False  # Image input
    streaming: bool = False  # Stream responses
    system_message: bool = True  # Supports system role

    # Quirks
    max_tools: int = 128  # Max tools in single request
    parallel_tool_calls: bool = True  # Can call multiple tools at once

# Capability matrix by provider
CAPABILITIES: dict[LLMProvider, ProviderCapabilities] = {
    LLMProvider.GEMINI: ProviderCapabilities(
        tools=True,
        json_schema=True,
        vision=True,
        streaming=True,
        system_message=True,
        max_tools=128,
        parallel_tool_calls=True,
    ),
    LLMProvider.OPENAI: ProviderCapabilities(
        tools=True,
        json_schema=True,
        vision=True,
        streaming=True,
        system_message=True,
        max_tools=128,
        parallel_tool_calls=True,
    ),
    LLMProvider.ANTHROPIC: ProviderCapabilities(
        tools=True,
        json_schema=True,
        vision=True,
        streaming=True,
        system_message=True,
        max_tools=128,
        parallel_tool_calls=True,
    ),
}

def get_capabilities(provider: LLMProvider) -> ProviderCapabilities:
    """Get capabilities for a provider."""
    return CAPABILITIES.get(provider, ProviderCapabilities())

def supports_feature(provider: LLMProvider, feature: str) -> bool:
    """Check if provider supports a specific feature."""
    caps = get_capabilities(provider)
    return getattr(caps, feature, False)
```
  </action>
  <verify>python -c "from src.llm.capabilities import get_capabilities, LLMProvider; print(get_capabilities(LLMProvider.GEMINI))"</verify>
  <done>Capability matrix with feature detection functions</done>
</task>

<task type="auto">
  <name>Task 3: Create llm package with exports</name>
  <files>src/llm/__init__.py</files>
  <action>
Create src/llm/__init__.py with all type exports.

Export:
- LLMProvider, MessageRole, FinishReason (enums)
- Message, ToolCall, TokenUsage, LLMResult, LLMConfig (types)
- ProviderCapabilities, get_capabilities, supports_feature (capabilities)

Pattern: from src.llm import LLMProvider, Message, LLMResult
  </action>
  <verify>python -c "from src.llm import LLMProvider, Message, LLMResult, get_capabilities; print('package ok')"</verify>
  <done>Clean package exports for all core types</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `python -c "from src.llm import LLMProvider, Message, LLMResult"` succeeds
- [ ] `python -c "from src.llm import get_capabilities, supports_feature"` succeeds
- [ ] All types are Pydantic models (serializable)
- [ ] No import errors
</verification>

<success_criteria>

- All tasks completed
- Core types define the contract for all adapters
- Capability matrix ready for feature detection
- Types are serializable (Pydantic)
</success_criteria>

<output>
After completion, create `.planning/phases/03-llm-integration/03-01-SUMMARY.md`
</output>
