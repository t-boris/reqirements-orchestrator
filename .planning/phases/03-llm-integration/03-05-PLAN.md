---
phase: 03-llm-integration
plan: 05
type: execute
wave: 3
depends_on: ["03-02", "03-03", "03-04"]
files_modified: [src/llm/client.py, src/llm/factory.py, src/llm/__init__.py]
autonomous: true
---

<objective>
Create UnifiedChatClient and LLMFactory for provider-agnostic LLM access.

Purpose: Provide single interface for business logic to use any LLM provider.
Output: UnifiedChatClient that wraps adapters, LLMFactory that creates clients.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

Architecture decisions:
- 3-layer: Factory -> UnifiedClient -> Adapters
- Capability matrix for feature detection
- Auto-detect provider from model name

Prior plans provide:
- 03-01: Types (LLMProvider, Message, LLMResult, LLMConfig)
- 03-02: GeminiAdapter
- 03-03: OpenAIAdapter
- 03-04: AnthropicAdapter

@src/llm/types.py
@src/llm/adapters/base.py
@src/config/settings.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create LLM factory</name>
  <files>src/llm/factory.py</files>
  <action>
Create factory that instantiates the correct adapter based on provider/model.

```python
from src.llm.types import LLMProvider, LLMConfig
from src.llm.adapters.base import BaseAdapter
from src.config import get_settings

def detect_provider(model: str) -> LLMProvider:
    """Detect provider from model name."""
    model_lower = model.lower()
    if model_lower.startswith("gemini"):
        return LLMProvider.GEMINI
    elif model_lower.startswith("gpt") or model_lower.startswith("o1"):
        return LLMProvider.OPENAI
    elif model_lower.startswith("claude"):
        return LLMProvider.ANTHROPIC
    else:
        # Default to Gemini (project default)
        return LLMProvider.GEMINI

def create_adapter(config: LLMConfig) -> BaseAdapter:
    """Create adapter instance for the specified provider."""
    if config.provider == LLMProvider.GEMINI:
        from src.llm.adapters.gemini import GeminiAdapter
        return GeminiAdapter(config)
    elif config.provider == LLMProvider.OPENAI:
        from src.llm.adapters.openai import OpenAIAdapter
        return OpenAIAdapter(config)
    elif config.provider == LLMProvider.ANTHROPIC:
        from src.llm.adapters.anthropic import AnthropicAdapter
        return AnthropicAdapter(config)
    else:
        raise ValueError(f"Unknown provider: {config.provider}")

def get_default_model(provider: LLMProvider) -> str:
    """Get default model for a provider."""
    defaults = {
        LLMProvider.GEMINI: "gemini-1.5-flash",
        LLMProvider.OPENAI: "gpt-4o-mini",
        LLMProvider.ANTHROPIC: "claude-3-5-sonnet-latest",
    }
    return defaults.get(provider, "gemini-1.5-flash")
```
  </action>
  <verify>python -c "from src.llm.factory import detect_provider, create_adapter, LLMProvider; print(detect_provider('gemini-1.5-pro'))"</verify>
  <done>LLM factory with provider detection and adapter creation</done>
</task>

<task type="auto">
  <name>Task 2: Create UnifiedChatClient</name>
  <files>src/llm/client.py</files>
  <action>
Create UnifiedChatClient that provides the main interface for business logic.

```python
from typing import Optional
from pydantic import BaseModel

from src.llm.types import (
    LLMProvider, LLMConfig, LLMResult, Message
)
from src.llm.adapters.base import BaseAdapter, ToolDefinition
from src.llm.factory import detect_provider, create_adapter, get_default_model
from src.llm.capabilities import get_capabilities, supports_feature
from src.config import get_settings

class UnifiedChatClient:
    """Provider-agnostic LLM client.

    Usage:
        client = UnifiedChatClient()  # Uses default from settings
        client = UnifiedChatClient(model="gpt-4o")  # Auto-detects OpenAI
        client = UnifiedChatClient(provider=LLMProvider.ANTHROPIC)

        result = await client.invoke(messages)
        result = await client.invoke(messages, tools=[...])
        result = await client.invoke(messages, response_schema=MyModel)
    """

    def __init__(
        self,
        model: str | None = None,
        provider: LLMProvider | None = None,
        temperature: float = 0.7,
        max_tokens: int = 4096,
        timeout_seconds: float = 30.0,
        api_key: str | None = None,
    ):
        settings = get_settings()

        # Determine provider and model
        if provider is None and model is None:
            # Use project default
            model = settings.default_llm_model
            provider = detect_provider(model)
        elif provider is None:
            provider = detect_provider(model)
        elif model is None:
            model = get_default_model(provider)

        self.config = LLMConfig(
            provider=provider,
            model=model,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout_seconds=timeout_seconds,
            api_key=api_key,
        )

        self._adapter: BaseAdapter | None = None

    @property
    def adapter(self) -> BaseAdapter:
        """Lazy-load adapter."""
        if self._adapter is None:
            self._adapter = create_adapter(self.config)
        return self._adapter

    @property
    def provider(self) -> LLMProvider:
        return self.config.provider

    @property
    def model(self) -> str:
        return self.config.model

    def supports(self, feature: str) -> bool:
        """Check if current provider supports a feature."""
        return supports_feature(self.config.provider, feature)

    async def invoke(
        self,
        messages: list[Message],
        tools: list[ToolDefinition] | None = None,
        response_schema: type[BaseModel] | None = None,
    ) -> LLMResult:
        """Send messages and get unified result."""
        # Validate feature support
        if tools and not self.supports("tools"):
            raise ValueError(f"{self.provider} does not support tool calling")
        if response_schema and not self.supports("json_schema"):
            raise ValueError(f"{self.provider} does not support structured output")

        return await self.adapter.invoke(
            messages=messages,
            tools=tools,
            response_schema=response_schema,
        )

    async def chat(self, user_message: str, system_message: str | None = None) -> str:
        """Simple chat interface - returns text only."""
        messages = []
        if system_message:
            messages.append(Message(role="system", content=system_message))
        messages.append(Message(role="user", content=user_message))

        result = await self.invoke(messages)
        return result.text


# Convenience function
def get_llm(
    model: str | None = None,
    provider: LLMProvider | None = None,
    **kwargs
) -> UnifiedChatClient:
    """Get an LLM client.

    Usage:
        llm = get_llm()  # Project default
        llm = get_llm("gpt-4o")  # OpenAI
        llm = get_llm(provider=LLMProvider.ANTHROPIC)  # Anthropic default

        result = await llm.invoke(messages)
    """
    return UnifiedChatClient(model=model, provider=provider, **kwargs)
```
  </action>
  <verify>python -c "from src.llm.client import UnifiedChatClient, get_llm; print('client ok')"</verify>
  <done>UnifiedChatClient with provider-agnostic interface</done>
</task>

<task type="auto">
  <name>Task 3: Update llm package exports</name>
  <files>src/llm/__init__.py</files>
  <action>
Add client and factory exports to src/llm/__init__.py:

From client:
- UnifiedChatClient
- get_llm

From factory:
- detect_provider
- create_adapter
- get_default_model

Keep existing exports from 03-01.

Main interface: from src.llm import get_llm, UnifiedChatClient
  </action>
  <verify>python -c "from src.llm import get_llm, UnifiedChatClient, detect_provider; print('exports ok')"</verify>
  <done>All LLM components available from src.llm</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `python -c "from src.llm import get_llm, UnifiedChatClient"` succeeds
- [ ] detect_provider correctly identifies all 3 providers
- [ ] UnifiedChatClient lazy-loads adapter
- [ ] No import errors
</verification>

<success_criteria>

- All tasks completed
- UnifiedChatClient provides clean interface for business logic
- Factory handles provider detection and adapter creation
- Capability checks prevent unsupported operations
</success_criteria>

<output>
After completion, create `.planning/phases/03-llm-integration/03-05-SUMMARY.md`
</output>
