---
phase: 03-llm-integration
plan: 06
type: execute
wave: 2
depends_on: ["03-01"]
files_modified: [src/llm/prompts/__init__.py, src/llm/prompts/templates.py, src/llm/prompts/builder.py, src/llm/prompts/overlays.py]
autonomous: true
---

<objective>
Create prompt system with base templates and provider-specific overlays.

Purpose: Manage prompts that work consistently across providers with provider-specific optimizations.
Output: PromptBuilder with base templates, provider overlays, and assembled prompt logging.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

Architecture decisions:
- Base prompt + provider-specific overlays
- Simple templating (Jinja2/format), not DSL
- Log assembled prompts with secret redaction
- Prompts for: extraction, validation, questioning

Key context from PROJECT.md:
- Bot acts as proactive business analyst
- Drives conversations to gather complete requirements
- Type-specific schemas: Epic, Story, Task, Bug

@src/schemas/ticket.py
@src/llm/types.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create base prompt templates</name>
  <files>src/llm/prompts/templates.py</files>
  <action>
Create base prompt templates for analyst behaviors.

```python
"""Base prompt templates for the Jira analyst agent."""

# System prompt - the agent's identity
ANALYST_SYSTEM_BASE = """You are a proactive Jira ticket analyst. Your role is to:
1. Extract requirements from conversations
2. Identify missing information
3. Ask clarifying questions until requirements are complete
4. Never create half-baked tickets

You work with these ticket types:
- Epic: High-level features (needs: summary, description)
- Story: User-facing features (needs: summary, description, acceptance_criteria)
- Task: Technical work (needs: summary, description)
- Bug: Defects (needs: summary, description, steps_to_reproduce, expected_behavior, actual_behavior)

Be conversational, not robotic. Ask one question at a time when possible."""

# Extraction prompt - parse messages to update draft
EXTRACTION_TEMPLATE = """Current ticket type: {ticket_type}
Current draft:
{draft_json}

Missing fields: {missing_fields}

---

Based on this conversation, extract and update the ticket fields:

{messages}

Return the updated ticket as JSON matching the {ticket_type} schema. Only include fields you can confidently extract from the conversation."""

# Validation prompt - check completeness
VALIDATION_TEMPLATE = """Current ticket type: {ticket_type}
Current draft:
{draft_json}

Missing fields: {missing_fields}

---

Review the current ticket draft and determine:
1. Is the ticket complete enough to create in Jira?
2. What critical information is still missing?
3. What's the single most important question to ask next?

Be specific about what's missing and why it matters for this ticket type."""

# Questioning prompt - generate clarifying questions
QUESTIONING_TEMPLATE = """Current ticket type: {ticket_type}
Current draft:
{draft_json}

Missing fields: {missing_fields}

---

Generate 1-2 clarifying questions to gather the missing information.

Questions should be:
- Specific and actionable
- Prioritized by importance
- Natural and conversational

Focus on: {missing_fields}"""

# Preview prompt - format ticket for approval
PREVIEW_TEMPLATE = """Here's the ticket I'm about to create:

**Type:** {ticket_type}
**Summary:** {summary}
**Description:**
{description}

{type_specific_fields}

Does this look correct? Reply "create" to proceed or let me know what needs to change."""
```
  </action>
  <verify>python -c "from src.llm.prompts.templates import ANALYST_SYSTEM_BASE, EXTRACTION_TEMPLATE; print('templates ok')"</verify>
  <done>Base prompt templates for all analyst behaviors</done>
</task>

<task type="auto">
  <name>Task 2: Create provider overlays</name>
  <files>src/llm/prompts/overlays.py</files>
  <action>
Create provider-specific prompt modifications.

Same prompt can behave differently across providers. Overlays add provider-specific instructions.

```python
"""Provider-specific prompt overlays.

Different LLMs respond better to different instruction styles.
Overlays modify base prompts for optimal behavior per provider.
"""

from src.llm.types import LLMProvider

# Provider-specific system prompt additions
SYSTEM_OVERLAYS: dict[LLMProvider, str] = {
    LLMProvider.GEMINI: """
When extracting JSON, be precise with field names and types.
Use the exact field names from the schema.""",

    LLMProvider.OPENAI: """
Follow the JSON schema exactly. Do not add extra fields.
When uncertain, ask for clarification rather than guessing.""",

    LLMProvider.ANTHROPIC: """
Think step by step when extracting information.
Explain your reasoning briefly before providing the JSON output.""",
}

# Provider-specific extraction hints
EXTRACTION_OVERLAYS: dict[LLMProvider, str] = {
    LLMProvider.GEMINI: """
Output format: Return ONLY valid JSON, no markdown code blocks.""",

    LLMProvider.OPENAI: """
Output format: Return valid JSON wrapped in ```json``` code block.""",

    LLMProvider.ANTHROPIC: """
First briefly note what you extracted, then return the JSON in a code block.""",
}

def get_system_overlay(provider: LLMProvider) -> str:
    """Get system prompt overlay for provider."""
    return SYSTEM_OVERLAYS.get(provider, "")

def get_extraction_overlay(provider: LLMProvider) -> str:
    """Get extraction prompt overlay for provider."""
    return EXTRACTION_OVERLAYS.get(provider, "")

def apply_overlay(base_prompt: str, overlay: str) -> str:
    """Apply overlay to base prompt."""
    if not overlay:
        return base_prompt
    return f"{base_prompt}\n\n{overlay.strip()}"
```
  </action>
  <verify>python -c "from src.llm.prompts.overlays import get_system_overlay, LLMProvider; print(get_system_overlay(LLMProvider.GEMINI)[:50])"</verify>
  <done>Provider-specific overlays for Gemini, OpenAI, Anthropic</done>
</task>

<task type="auto">
  <name>Task 3: Create prompt builder</name>
  <files>src/llm/prompts/builder.py</files>
  <action>
Create PromptBuilder that assembles prompts with overlays and logging.

```python
"""Prompt builder with provider overlays and logging."""

import json
import logging
import hashlib
from typing import Any, Optional

from src.llm.types import LLMProvider, Message, MessageRole
from src.llm.prompts.templates import (
    ANALYST_SYSTEM_BASE,
    EXTRACTION_TEMPLATE,
    VALIDATION_TEMPLATE,
    QUESTIONING_TEMPLATE,
    PREVIEW_TEMPLATE,
)
from src.llm.prompts.overlays import (
    get_system_overlay,
    get_extraction_overlay,
    apply_overlay,
)

logger = logging.getLogger(__name__)

# Fields to redact in logs
REDACT_FIELDS = {"api_key", "token", "secret", "password", "credential"}

def _redact_secrets(text: str) -> str:
    """Redact potential secrets from text for logging."""
    # Simple redaction - in production, use more sophisticated detection
    for field in REDACT_FIELDS:
        if field in text.lower():
            # Redact values that look like secrets
            import re
            pattern = rf'({field}["\']?\s*[:=]\s*["\']?)([^"\'\s]+)'
            text = re.sub(pattern, r'\1[REDACTED]', text, flags=re.IGNORECASE)
    return text

def _prompt_hash(text: str) -> str:
    """Generate short hash for prompt identification."""
    return hashlib.sha256(text.encode()).hexdigest()[:8]

class PromptBuilder:
    """Build prompts with provider-specific overlays.

    Usage:
        builder = PromptBuilder(provider=LLMProvider.GEMINI)
        messages = builder.build_extraction_prompt(
            ticket_type="Story",
            draft={"summary": "..."},
            missing_fields=["acceptance_criteria"],
            conversation="User said...",
        )
    """

    def __init__(self, provider: LLMProvider):
        self.provider = provider

    def _log_prompt(self, prompt_type: str, content: str):
        """Log assembled prompt with redaction."""
        redacted = _redact_secrets(content)
        prompt_hash = _prompt_hash(content)
        logger.debug(
            f"Assembled {prompt_type} prompt",
            extra={
                "prompt_type": prompt_type,
                "prompt_hash": prompt_hash,
                "provider": self.provider.value,
                "content_preview": redacted[:200] + "..." if len(redacted) > 200 else redacted,
            }
        )

    def build_system_prompt(self) -> str:
        """Build system prompt with provider overlay."""
        overlay = get_system_overlay(self.provider)
        prompt = apply_overlay(ANALYST_SYSTEM_BASE, overlay)
        self._log_prompt("system", prompt)
        return prompt

    def build_extraction_prompt(
        self,
        ticket_type: str,
        draft: dict | None,
        missing_fields: list[str],
        conversation: str,
    ) -> list[Message]:
        """Build extraction prompt messages."""
        system = self.build_system_prompt()

        user_content = EXTRACTION_TEMPLATE.format(
            ticket_type=ticket_type,
            draft_json=json.dumps(draft, indent=2) if draft else "{}",
            missing_fields=", ".join(missing_fields) if missing_fields else "none",
            messages=conversation,
        )

        # Apply extraction overlay
        overlay = get_extraction_overlay(self.provider)
        user_content = apply_overlay(user_content, overlay)

        self._log_prompt("extraction", user_content)

        return [
            Message(role=MessageRole.SYSTEM, content=system),
            Message(role=MessageRole.USER, content=user_content),
        ]

    def build_validation_prompt(
        self,
        ticket_type: str,
        draft: dict | None,
        missing_fields: list[str],
    ) -> list[Message]:
        """Build validation prompt messages."""
        system = self.build_system_prompt()

        user_content = VALIDATION_TEMPLATE.format(
            ticket_type=ticket_type,
            draft_json=json.dumps(draft, indent=2) if draft else "{}",
            missing_fields=", ".join(missing_fields) if missing_fields else "none",
        )

        self._log_prompt("validation", user_content)

        return [
            Message(role=MessageRole.SYSTEM, content=system),
            Message(role=MessageRole.USER, content=user_content),
        ]

    def build_questioning_prompt(
        self,
        ticket_type: str,
        draft: dict | None,
        missing_fields: list[str],
    ) -> list[Message]:
        """Build questioning prompt messages."""
        system = self.build_system_prompt()

        user_content = QUESTIONING_TEMPLATE.format(
            ticket_type=ticket_type,
            draft_json=json.dumps(draft, indent=2) if draft else "{}",
            missing_fields=", ".join(missing_fields) if missing_fields else "none",
        )

        self._log_prompt("questioning", user_content)

        return [
            Message(role=MessageRole.SYSTEM, content=system),
            Message(role=MessageRole.USER, content=user_content),
        ]


def get_prompt_builder(provider: LLMProvider) -> PromptBuilder:
    """Get a prompt builder for the specified provider."""
    return PromptBuilder(provider)
```
  </action>
  <verify>python -c "from src.llm.prompts.builder import PromptBuilder, LLMProvider; b = PromptBuilder(LLMProvider.GEMINI); print('builder ok')"</verify>
  <done>PromptBuilder with overlays and logging</done>
</task>

<task type="auto">
  <name>Task 4: Create prompts package with exports</name>
  <files>src/llm/prompts/__init__.py</files>
  <action>
Create src/llm/prompts/__init__.py with exports:

From templates:
- ANALYST_SYSTEM_BASE
- EXTRACTION_TEMPLATE
- VALIDATION_TEMPLATE
- QUESTIONING_TEMPLATE
- PREVIEW_TEMPLATE

From builder:
- PromptBuilder
- get_prompt_builder

From overlays:
- get_system_overlay
- get_extraction_overlay

Pattern: from src.llm.prompts import PromptBuilder, get_prompt_builder
  </action>
  <verify>python -c "from src.llm.prompts import PromptBuilder, get_prompt_builder; print('prompts package ok')"</verify>
  <done>Prompts package with clean exports</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `python -c "from src.llm.prompts import PromptBuilder, get_prompt_builder"` succeeds
- [ ] PromptBuilder builds messages with overlays
- [ ] Logging includes prompt hash and redacted content
- [ ] No import errors
</verification>

<success_criteria>

- All tasks completed
- Base templates for all analyst behaviors
- Provider overlays customize prompts per provider
- Assembled prompts logged with secret redaction
</success_criteria>

<output>
After completion, create `.planning/phases/03-llm-integration/03-06-SUMMARY.md`
</output>
