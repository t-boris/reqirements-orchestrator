---
phase: 03-llm-integration
plan: 02
type: execute
wave: 1
depends_on: []
files_modified: [src/llm/factory.py, src/llm/__init__.py]
autonomous: true
---

<objective>
Create LLM provider abstraction layer supporting multiple providers (Gemini default, OpenAI optional).

Purpose: Allow swapping LLM providers without changing agent code.
Output: get_llm() factory function that returns appropriate provider based on config.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

Prior work:
- Settings has default_llm_model = "gemini-1.5-flash"
- 03-01 creates get_gemini_llm() in src/llm/gemini.py

Design decision from PROJECT.md:
- Configurable LLM with Gemini default
- Support multiple providers for flexibility

@src/config/settings.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create LLM factory module</name>
  <files>src/llm/factory.py</files>
  <action>
Create LLM provider factory that abstracts provider selection.

Implementation:
1. Define LLMProvider enum: GEMINI, OPENAI (for future)
2. Create get_llm() factory function
3. Detect provider from model name prefix (gemini-* → Gemini, gpt-* → OpenAI)
4. Return appropriate LangChain chat model

Pattern:
```python
from enum import Enum
from typing import Literal
from langchain_core.language_models import BaseChatModel
from src.config import get_settings

class LLMProvider(str, Enum):
    GEMINI = "gemini"
    OPENAI = "openai"

def detect_provider(model: str) -> LLMProvider:
    """Detect provider from model name."""
    if model.startswith("gemini"):
        return LLMProvider.GEMINI
    elif model.startswith("gpt"):
        return LLMProvider.OPENAI
    else:
        # Default to Gemini
        return LLMProvider.GEMINI

def get_llm(
    model: str | None = None,
    temperature: float = 0.7,
    **kwargs
) -> BaseChatModel:
    """Factory function to get LLM by model name."""
    settings = get_settings()
    model = model or settings.default_llm_model
    provider = detect_provider(model)

    if provider == LLMProvider.GEMINI:
        from src.llm.gemini import get_gemini_llm
        return get_gemini_llm(model=model, temperature=temperature, **kwargs)
    elif provider == LLMProvider.OPENAI:
        # Future: add OpenAI support
        raise NotImplementedError("OpenAI provider not yet implemented")
    else:
        raise ValueError(f"Unknown provider: {provider}")
```

This allows code to use get_llm() without caring about the specific provider.
  </action>
  <verify>python -c "from src.llm.factory import get_llm, LLMProvider, detect_provider; print('imports ok')"</verify>
  <done>LLM factory with provider abstraction available</done>
</task>

<task type="auto">
  <name>Task 2: Update llm package exports</name>
  <files>src/llm/__init__.py</files>
  <action>
Add factory exports to src/llm/__init__.py:
- get_llm (main entry point)
- LLMProvider
- detect_provider

Keep existing gemini exports from 03-01.

Pattern: from src.llm import get_llm
  </action>
  <verify>python -c "from src.llm import get_llm, LLMProvider; print('exports ok')"</verify>
  <done>Factory functions available from src.llm</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `python -c "from src.llm import get_llm, LLMProvider"` succeeds
- [ ] detect_provider("gemini-1.5-flash") returns GEMINI
- [ ] No import errors
</verification>

<success_criteria>

- All tasks completed
- LLM factory abstracts provider selection
- Agent code can use get_llm() without knowing provider
- Clean imports from src.llm
</success_criteria>

<output>
After completion, create `.planning/phases/03-llm-integration/03-02-SUMMARY.md`
</output>
