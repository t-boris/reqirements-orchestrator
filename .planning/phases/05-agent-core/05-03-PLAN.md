---
phase: 05-agent-core
plan: 03
type: execute
wave: 3
depends_on: ["05-02"]
files_modified: [src/graph/nodes/validation.py, src/graph/nodes/decision.py, src/graph/nodes/__init__.py, src/graph/graph.py]
autonomous: true
---

<objective>
Create validation and decision nodes for the PM-machine workflow.

Purpose: Complete the product loop with LLM-first validation and ASK/PREVIEW/READY_TO_CREATE routing.
Output: Validation node with detailed report, Decision node with smart batching.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-agent-core/05-CONTEXT.md
@.planning/phases/05-agent-core/05-01-SUMMARY.md
@.planning/phases/05-agent-core/05-02-SUMMARY.md

From CONTEXT.md:
- Validation: LLM-first with rule-based fallback. Outputs detailed report: missing_fields[], conflicts[], suggestions[]
- Decision: Three outcomes (ASK / PREVIEW / EXECUTE), prioritizes most impactful issues first
- Smart batching for questions (immediate if urgent, else batch related)
- EXECUTE deferred to Phase 7 (update state to READY_TO_CREATE only)

@src/graph/graph.py (after 05-02)
@src/graph/nodes/extraction.py (after 05-02)
@src/schemas/draft.py (after 05-01)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create validation node with detailed report</name>
  <files>src/graph/nodes/validation.py</files>
  <action>
Create src/graph/nodes/validation.py:

```python
"""Validation node - checks draft completeness and detects conflicts.

LLM-first approach: Uses LLM for semantic validation,
falls back to rule-based checks for structural requirements.

Output: ValidationReport with missing_fields[], conflicts[], suggestions[]
"""
import json
import logging
from typing import Any, Optional
from pydantic import BaseModel, Field

from src.schemas.state import AgentState, AgentPhase
from src.schemas.draft import TicketDraft
from src.llm import get_llm

logger = logging.getLogger(__name__)


class ValidationReport(BaseModel):
    """Detailed validation report for decision node."""
    is_valid: bool = False
    missing_fields: list[str] = Field(default_factory=list)
    conflicts: list[str] = Field(default_factory=list)
    suggestions: list[str] = Field(default_factory=list)
    quality_score: int = 0  # 0-100, for prioritization


VALIDATION_PROMPT = '''You are validating a Jira ticket draft for completeness and quality.

Draft:
{draft_json}

Analyze this draft and provide a validation report as JSON:

{{
  "is_valid": true/false,  // Ready for preview?
  "missing_fields": ["field1", "field2"],  // Required but empty/insufficient
  "conflicts": ["description of conflict"],  // Contradictory information
  "suggestions": ["improvement suggestion"],  // Optional improvements
  "quality_score": 0-100  // Overall readiness score
}}

Minimum requirements for is_valid=true:
- title: Clear, concise (not empty)
- problem: Describes what needs solving (not empty)
- acceptance_criteria: At least 1 testable criterion

Check for:
- Logical conflicts between stated requirements
- Ambiguous or vague descriptions
- Missing context that would be needed

JSON response:'''


def rule_based_validation(draft: TicketDraft) -> ValidationReport:
    """Fallback rule-based validation.

    Used if LLM validation fails.
    """
    report = ValidationReport()

    # Check required fields
    if not draft.title.strip():
        report.missing_fields.append("title")
    if not draft.problem.strip():
        report.missing_fields.append("problem")
    if not draft.acceptance_criteria:
        report.missing_fields.append("acceptance_criteria (at least one)")

    # Check for constraint conflicts (same key, different values)
    seen_constraints = {}
    for c in draft.constraints:
        if c.key in seen_constraints and seen_constraints[c.key] != c.value:
            report.conflicts.append(
                f"Conflicting values for {c.key}: '{seen_constraints[c.key]}' vs '{c.value}'"
            )
        seen_constraints[c.key] = c.value

    # Calculate score
    total_fields = 3  # title, problem, AC
    filled_fields = sum([
        bool(draft.title.strip()),
        bool(draft.problem.strip()),
        bool(draft.acceptance_criteria),
    ])
    report.quality_score = int((filled_fields / total_fields) * 100)

    report.is_valid = len(report.missing_fields) == 0
    return report


async def validation_node(state: AgentState) -> dict[str, Any]:
    """Validate draft and produce detailed report.

    - Uses LLM for semantic validation
    - Falls back to rule-based if LLM fails
    - Stores report in state for decision node

    Returns partial state update.
    """
    draft = state.get("draft")
    step_count = state.get("step_count", 0)

    if not draft:
        logger.warning("No draft to validate")
        return {
            "step_count": step_count + 1,
            "phase": AgentPhase.COLLECTING,
            "validation_report": ValidationReport(
                missing_fields=["draft (no content yet)"]
            ).model_dump(),
        }

    # Try LLM validation first
    try:
        draft_json = draft.model_dump_json(
            exclude={"evidence_links", "created_at", "updated_at", "id"}
        )
        prompt = VALIDATION_PROMPT.format(draft_json=draft_json)

        llm = get_llm()
        result = await llm.chat(prompt)
        response_text = result.content.strip()

        # Parse JSON response
        if response_text.startswith("```"):
            response_text = response_text.split("```")[1]
            if response_text.startswith("json"):
                response_text = response_text[4:]
            response_text = response_text.strip()

        report_data = json.loads(response_text)
        report = ValidationReport(**report_data)

        logger.info(
            "LLM validation complete",
            extra={
                "is_valid": report.is_valid,
                "missing_count": len(report.missing_fields),
                "quality_score": report.quality_score,
            }
        )

    except Exception as e:
        logger.warning(f"LLM validation failed, using rule-based: {e}")
        report = rule_based_validation(draft)

    # Determine next phase
    if report.is_valid:
        next_phase = AgentPhase.VALIDATING  # Move to decision
    else:
        next_phase = AgentPhase.COLLECTING  # Need more info

    return {
        "step_count": step_count + 1,
        "phase": next_phase,
        "validation_report": report.model_dump(),
    }
```
  </action>
  <verify>python -c "from src.graph.nodes.validation import validation_node, ValidationReport; print('validation ok')"</verify>
  <done>Validation node with LLM-first approach and rule-based fallback</done>
</task>

<task type="auto">
  <name>Task 2: Create decision node with ASK/PREVIEW/READY_TO_CREATE</name>
  <files>src/graph/nodes/decision.py</files>
  <action>
Create src/graph/nodes/decision.py:

```python
"""Decision node - routes to ASK, PREVIEW, or READY_TO_CREATE.

Prioritizes most impactful issues first.
Smart batching: immediate if urgent, else batch related questions.

EXECUTE is deferred to Phase 7 - only sets state to READY_TO_CREATE.
"""
import logging
from typing import Any, Literal
from pydantic import BaseModel, Field

from src.schemas.state import AgentState, AgentPhase

logger = logging.getLogger(__name__)


class DecisionResult(BaseModel):
    """Result of decision node processing."""
    action: Literal["ask", "preview", "ready_to_create"]
    questions: list[str] = Field(default_factory=list)  # For ASK action
    reason: str = ""  # Why this decision


def prioritize_issues(
    missing_fields: list[str],
    conflicts: list[str],
    suggestions: list[str],
) -> list[str]:
    """Prioritize issues by impact.

    Order: conflicts (blockers) > missing required > suggestions (nice-to-have)
    Returns list of questions/issues, most impactful first.
    """
    questions = []

    # Conflicts are blockers - ask first
    for conflict in conflicts:
        questions.append(f"I found a conflict: {conflict}. How should we resolve this?")

    # Missing required fields
    field_questions = {
        "title": "What should be the title/summary for this ticket?",
        "problem": "What problem are we trying to solve?",
        "acceptance_criteria": "What are the acceptance criteria? How will we know this is done?",
    }
    for field in missing_fields:
        # Extract base field name
        base_field = field.split(" ")[0].strip("()")
        if base_field in field_questions:
            questions.append(field_questions[base_field])
        else:
            questions.append(f"Please provide: {field}")

    return questions


def batch_questions(questions: list[str], max_batch: int = 3) -> list[str]:
    """Batch related questions together.

    Returns at most max_batch questions to avoid overwhelming user.
    Most impactful questions first (already prioritized).
    """
    return questions[:max_batch]


async def decision_node(state: AgentState) -> dict[str, Any]:
    """Decide next action: ASK, PREVIEW, or READY_TO_CREATE.

    Logic:
    1. If validation passed (is_valid=True) -> PREVIEW
    2. If conflicts exist -> ASK (prioritize conflicts)
    3. If missing fields -> ASK (batch questions)
    4. If approved -> READY_TO_CREATE

    Returns partial state update with decision result.
    """
    draft = state.get("draft")
    validation_report = state.get("validation_report", {})
    step_count = state.get("step_count", 0)
    phase = state.get("phase", AgentPhase.COLLECTING)

    # Check if already approved (would be set by approval handler)
    if phase == AgentPhase.READY_TO_CREATE:
        logger.info("Draft already approved, ready to create")
        return {
            "step_count": step_count + 1,
            "decision_result": DecisionResult(
                action="ready_to_create",
                reason="Draft approved by user",
            ).model_dump(),
        }

    # Get validation details
    is_valid = validation_report.get("is_valid", False)
    missing_fields = validation_report.get("missing_fields", [])
    conflicts = validation_report.get("conflicts", [])
    suggestions = validation_report.get("suggestions", [])

    # Decision logic
    if is_valid and not conflicts:
        # Ready for preview
        logger.info("Draft valid, showing preview")
        return {
            "step_count": step_count + 1,
            "phase": AgentPhase.AWAITING_USER,
            "decision_result": DecisionResult(
                action="preview",
                reason="Draft meets minimum requirements",
            ).model_dump(),
        }

    # Need to ask questions
    questions = prioritize_issues(missing_fields, conflicts, suggestions)
    batched = batch_questions(questions)

    logger.info(
        "Asking user for more information",
        extra={
            "total_issues": len(questions),
            "batch_size": len(batched),
        }
    )

    return {
        "step_count": step_count + 1,
        "phase": AgentPhase.AWAITING_USER,
        "decision_result": DecisionResult(
            action="ask",
            questions=batched,
            reason=f"Need {len(missing_fields)} fields, {len(conflicts)} conflicts to resolve",
        ).model_dump(),
    }


def get_decision_action(state: AgentState) -> Literal["ask", "preview", "ready"]:
    """Get decision action from state for routing.

    Use in graph conditional edges.
    """
    result = state.get("decision_result", {})
    action = result.get("action", "ask")
    if action == "ready_to_create":
        return "ready"
    return action
```
  </action>
  <verify>python -c "from src.graph.nodes.decision import decision_node, DecisionResult; print('decision ok')"</verify>
  <done>Decision node with ASK/PREVIEW/READY_TO_CREATE routing and smart batching</done>
</task>

<task type="auto">
  <name>Task 3: Update graph with validation and decision nodes</name>
  <files>src/graph/graph.py, src/graph/nodes/__init__.py</files>
  <action>
Update src/graph/nodes/__init__.py to export validation_node and decision_node:
```python
from src.graph.nodes.extraction import extraction_node
from src.graph.nodes.validation import validation_node, ValidationReport
from src.graph.nodes.decision import decision_node, DecisionResult, get_decision_action

__all__ = [
    "extraction_node",
    "validation_node",
    "ValidationReport",
    "decision_node",
    "DecisionResult",
    "get_decision_action",
]
```

Update src/graph/graph.py to:
1. Import validation_node and decision_node
2. Replace validation_placeholder with real validation_node
3. Add decision node after validation
4. Add conditional edges from decision (ask -> extraction loop, preview -> END, ready -> END)
5. Update should_continue to route to validation when draft has content

The graph flow becomes:
```
START -> extraction -> should_continue -> validation -> decision -> (ask: extraction, preview: END, ready: END)
```
  </action>
  <verify>python -c "from src.graph import create_graph; g = create_graph(); print('updated graph ok')"</verify>
  <done>Graph updated with full validation and decision pipeline</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `python -c "from src.graph.nodes import validation_node, decision_node"` succeeds
- [ ] ValidationReport has missing_fields, conflicts, suggestions, quality_score
- [ ] Decision node returns ask/preview/ready_to_create
- [ ] Graph compiles with full pipeline
- [ ] No import errors
</verification>

<success_criteria>

- All tasks completed
- Validation node: LLM-first with rule-based fallback
- Validation report: detailed with missing_fields, conflicts, suggestions
- Decision node: ASK/PREVIEW/READY_TO_CREATE routing
- Questions batched (max 3) and prioritized by impact
- Graph has complete extract -> validate -> decide pipeline
</success_criteria>

<output>
After completion, create `.planning/phases/05-agent-core/05-03-SUMMARY.md`
</output>
